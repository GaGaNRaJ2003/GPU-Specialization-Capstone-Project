# %% [markdown]
"""
# CUDA-Accelerated Image Classification Capstone Project

This project demonstrates:
1. Verifying CUDA availability
2. Setting up GPU acceleration with CUDA
3. Loading and preprocessing image data
4. Building and training a CNN model with PyTorch
5. Evaluating model performance
6. Visualizing results
"""
# %%
# First, check if CUDA is available
import torch
import torchvision 
print("PyTorch version:", torch.__version__)
print("CUDA available:", torch.cuda.is_available())

if torch.cuda.is_available():
    device = torch.device("cuda")
    print("Current CUDA device:", torch.cuda.get_device_name(0))
    print("CUDA device count:", torch.cuda.device_count())
else:
    device = torch.device("cpu")
    print("CUDA not available, using CPU instead")

# %%
# Import necessary libraries
import numpy as np
import matplotlib.pyplot as plt
import time
from torch import nn, optim
from torch.utils.data import DataLoader, TensorDataset
from torchvision import datasets, transforms
import torch.nn.functional as F

# %%
# Define data transformations and load CIFAR-10 dataset
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

# Download training data
train_data = datasets.CIFAR10(
    root='data',
    train=True,
    download=True,
    transform=transform
)

# Download test data
test_data = datasets.CIFAR10(
    root='data',
    train=False,
    download=True,
    transform=transform
)

# Create data loaders
batch_size = 64
train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)

# Class names
class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',
               'dog', 'frog', 'horse', 'ship', 'truck']

# %%
# Visualize some training examples
def imshow(img):
    img = img / 2 + 0.5  # unnormalize
    npimg = img.numpy()
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()

# Get some random training images
dataiter = iter(train_loader)
images, labels = next(dataiter)

# Show images
imshow(torchvision.utils.make_grid(images[:4]))
# Print labels
print(' '.join(f'{class_names[labels[j]]:5s}' for j in range(4)))

# %%
# Define the CNN model
class CIFAR10_CNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.fc1 = nn.Linear(128 * 4 * 4, 512)
        self.fc2 = nn.Linear(512, 10)
        self.dropout = nn.Dropout(0.25)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = self.pool(F.relu(self.conv3(x)))
        x = torch.flatten(x, 1)  # flatten all dimensions except batch
        x = self.dropout(x)
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        return x

# Create model and move to GPU if available
model = CIFAR10_CNN().to(device)
print(model)

# %%
# Define loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# %%
# Training function with CUDA acceleration
def train(model, train_loader, criterion, optimizer, epochs=10):
    train_losses = []
    train_acc = []
    
    for epoch in range(epochs):
        running_loss = 0.0
        correct = 0
        total = 0
        
        # Start timer
        start_time = time.time()
        
        for images, labels in train_loader:
            # Move data to GPU if available
            images, labels = images.to(device), labels.to(device)
            
            # Zero the parameter gradients
            optimizer.zero_grad()
            
            # Forward pass
            outputs = model(images)
            loss = criterion(outputs, labels)
            
            # Backward pass and optimize
            loss.backward()
            optimizer.step()
            
            # Calculate statistics
            running_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
        
        # Calculate epoch statistics
        epoch_loss = running_loss / len(train_loader)
        epoch_acc = 100 * correct / total
        epoch_time = time.time() - start_time
        
        train_losses.append(epoch_loss)
        train_acc.append(epoch_acc)
        
        print(f'Epoch {epoch+1}/{epochs}, '
              f'Loss: {epoch_loss:.4f}, '
              f'Accuracy: {epoch_acc:.2f}%, '
              f'Time: {epoch_time:.2f}s')
    
    return train_losses, train_acc

# Train the model
print("Starting training...")
train_losses, train_acc = train(model, train_loader, criterion, optimizer, epochs=15)

# %%
# Plot training results
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(train_losses, label='Training Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Loss')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(train_acc, label='Training Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy (%)')
plt.title('Training Accuracy')
plt.legend()

plt.tight_layout()
plt.show()

# %%
# Evaluate the model on test data
def evaluate(model, test_loader):
    model.eval()  # Set model to evaluation mode
    correct = 0
    total = 0
    
    with torch.no_grad():
        for images, labels in test_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    
    accuracy = 100 * correct / total
    print(f'Test Accuracy: {accuracy:.2f}%')
    return accuracy

test_accuracy = evaluate(model, test_loader)

# %%
# Visualize some test predictions
def visualize_predictions(model, test_loader, num_images=6):
    model.eval()
    images, labels = next(iter(test_loader))
    images, labels = images.to(device), labels.to(device)
    
    with torch.no_grad():
        outputs = model(images[:num_images])
        _, preds = torch.max(outputs, 1)
    
    images = images.cpu()
    fig = plt.figure(figsize=(12, 8))
    for idx in range(num_images):
        ax = fig.add_subplot(2, 3, idx+1, xticks=[], yticks=[])
        imshow(images[idx])
        ax.set_title(f"Pred: {class_names[preds[idx]]}\nTrue: {class_names[labels[idx]]}",
                    color=("green" if preds[idx]==labels[idx] else "red"))
    plt.show()

visualize_predictions(model, test_loader)

# %%
# Benchmark performance with and without CUDA
def benchmark(model, device):
    dummy_input = torch.randn(64, 3, 32, 32, device=device)
    repetitions = 100
    
    # Warm-up
    for _ in range(10):
        _ = model(dummy_input)
    
    # Benchmark
    start_time = time.time()
    for _ in range(repetitions):
        _ = model(dummy_input)
    elapsed_time = (time.time() - start_time) / repetitions * 1000  # ms per batch
    
    print(f"Device: {device}, Average inference time: {elapsed_time:.2f} ms per batch")

if torch.cuda.is_available():
    print("\nBenchmarking with CUDA:")
    benchmark(model, 'cuda')
    
    print("\nBenchmarking with CPU:")
    cpu_model = CIFAR10_CNN().to('cpu')
    cpu_model.load_state_dict(model.state_dict())
    benchmark(cpu_model, 'cpu')
else:
    print("CUDA not available for benchmarking")
